# FastAPI Implementation Plan

## Overview
Implement FastAPI endpoints for artifact generation (flashcards, MCQs, insights) and LLM chat communication. Extract chat functionality from demos into production `llm_chat` module. Maintain session-based chat with RAG context and memory management.

## Project Structure

### New Directories
- `src/api/` - FastAPI application
  - `main.py` - FastAPI app instance, startup/shutdown lifecycle
  - `dependencies.py` - Shared dependency injection (RAG instances, generators, session manager)
  - `routes/` - API endpoint modules
    - `health.py` - Health check endpoint
    - `artifacts.py` - Flashcard, MCQ, Insights endpoints
    - `chat.py` - LLM chat endpoint
  - `models/` - Pydantic request/response models
    - `artifacts.py` - Artifact request/response schemas
    - `chat.py` - Chat request/response schemas

- `src/llm_chat/` - Production chat system (extracted from demos)
  - `session_manager.py` - In-memory session storage and cleanup
  - `chat_service.py` - Core chat logic (extracted from ContextChatSession)
  - `context_manager.py` - Conversation history and RAG context management

## Dependencies

### Add to requirements.txt
- `fastapi>=0.104.0` - Web framework
- `uvicorn[standard]>=0.24.0` - ASGI server
- `pydantic>=2.0.0` - Request/response validation

## API Endpoints

### Health Check
- **Endpoint**: `GET /health`
- **Purpose**: Lightweight health check for API availability
- **Response**: Minimal system status
  - `status`: "healthy" | "degraded" | "unhealthy"
  - `api`: dict (status, uptime_seconds)
  - Quick checks only (no heavy operations):
    - API running status
    - Component initialization status (from startup cache)
    - No RAG queries, no document counts, no generator calls
    - No active session enumeration

### Artifact Generation Endpoints
- **Base Path**: `/api/`
- **Endpoints**:
  - `POST /api/flashcards`
  - `POST /api/mcq`
  - `POST /api/insights`
- **Request Model**:
  - `topic` (string, required)
  - `num_items` (integer, optional, default=1)
- **Response**: Full artifact JSON (as generated by existing generators)
  - Includes: artifact_type, version, cards/questions/insights, provenance, metrics
  - Error artifacts passed through directly if generation fails
- **Error Handling**: Return existing generator error artifacts in API response

### Chat Endpoint
- **Endpoint**: `POST /api/chat`
- **Request Model**:
  - `message` (string, required)
  - `session_id` (string, optional - defaults to single global session)
- **Response Model**:
  - `answer` (string)
  - `session_id` (string)
  - `response_time` (float, seconds)
  - `conversation_length` (integer)
- **Error Handling**: Standardized error response format

### Session Management Endpoint
- **Endpoint**: `DELETE /api/chat/session/{session_id}`
- **Purpose**: Explicitly clear a chat session
- **Path Parameter**:
  - `session_id` (string, optional - defaults to single global session)
- **Response**: Success confirmation
- **Error Handling**: Standardized error response if session not found

## Session Management

### Storage (Simplified for Initial Implementation)
- **Type**: Single global session (expandable to dictionary later)
- **Current Approach**: One user, one session at a time
  - Single `ChatSession` instance managed by `ChatService`
  - No session_id lookup required initially
  - Session automatically created on first chat request
  - Simplified implementation for MVP
- **Future Expansion**: 
  - Migrate to dictionary: `{session_id: ChatSession}`
  - Add session_id generation and lookup
  - Support multiple concurrent sessions
- **Persistence**: Session kept until explicit clear or API shutdown
- **Cleanup**: No automatic expiration; manual cleanup on API shutdown
- **Session ID**: Optional parameter (defaults to global session)

### Session Lifecycle
1. Single global session created at startup or first chat request
2. Session persists conversation history
3. Session manages RAG context vector store (collection: "context_docs")
4. Session cleared via `DELETE /api/chat/session/{session_id}` or API shutdown
5. Session ID parameter optional (uses global session if not provided)

### Context Management
- Extract `ContextChatSession` logic from `context_llm_demo.py`
- Maintain conversation history (last 10 exchanges in memory)
- Summarize older exchanges (>8) and ingest into vector store
- Retrieve relevant context from vector store for current question
- Use conversation summary for key facts (names, topics)
- System prompt loaded from `src/demos/system_prompt.md`

## Startup Initialization

### FastAPI Lifespan
- **Startup**:
  - Initialize `BasicRAG` instance (for artifacts, collection: "persistant_docs")
  - Initialize all artifact generators (FlashcardGenerator, MCQGenerator, InsightsGenerator)
  - Initialize `ChatService` (session manager)
  - Load system prompt from `src/demos/system_prompt.md`
  - Log initialization status
- **Shutdown**:
  - Clear all chat sessions
  - Cleanup resources if needed

### Dependency Injection
- `get_rag_system()` - Returns shared BasicRAG instance
- `get_flashcard_generator()` - Returns FlashcardGenerator instance
- `get_mcq_generator()` - Returns MCQGenerator instance
- `get_insights_generator()` - Returns InsightsGenerator instance
- `get_chat_service()` - Returns ChatService instance

## Async Implementation

### Strategy
- Use async endpoints (FastAPI native)
- Wrap sync LLM calls with `asyncio.to_thread()` for blocking operations
- Keep existing sync gateway code unchanged
- Async enables concurrent request handling

### Endpoint Patterns
- Artifact endpoints: Async wrapper around sync generator calls
- Chat endpoint: Async wrapper around sync chat service calls
- Health endpoint: Async, lightweight checks

## Error Handling

### Standardized Error Response
```json
{
  "error": "Error message string",
  "code": "ERROR_CODE",
  "details": {}
}
```

### Error Sources
- **Artifact Generation**: Pass through existing generator error artifacts
- **Chat**: Convert exceptions to standardized format
- **Validation**: Pydantic validation errors (auto-handled by FastAPI)
- **System Errors**: 500-level errors with standardized format

## Logging Integration

### Log Points
- API request received (endpoint, session_id if applicable)
- API response sent (status, timing)
- Errors (full error details)
- Session creation/deletion
- System startup/shutdown

### Logger Configuration
- Use existing `logging_config.py` module
- Get logger via `get_logger(__name__)` pattern
- Include correlation IDs (session_id, request_id) in logs

## Pydantic Models

### Artifact Request
- `topic`: str (required)
- `num_items`: int (default=1, optional)
- No additional validation (min/max lengths, etc.)

### Artifact Response
- Dynamic based on artifact type (flashcards, mcq, insights)
- Full artifact JSON structure
- Error artifacts included in same structure

### Chat Request
- `message`: str (required)
- `session_id`: str (optional, defaults to global session)

### Chat Response
- `answer`: str
- `session_id`: str
- `response_time`: float
- `conversation_length`: int

### Health Response (Lightweight)
- `status`: str ("healthy" | "degraded" | "unhealthy")
- `api`: dict (status: "running", uptime_seconds: float)
- Component initialization status (from startup cache, no runtime checks):
  - `rag`: dict (initialized: bool)
  - `generators`: dict (flashcard: bool, mcq: bool, insights: bool)
  - `chat_service`: dict (initialized: bool)
- Note: No document counts, no active session enumeration, no generator calls

## Chat Service Implementation

### Extract from ContextChatSession
- `chat_with_context()` - Main chat method
- `get_conversation_context()` - Build context from history and RAG (with cached summary optimization)
- `add_to_history()` - Add exchange to conversation
- `_summarize_older_context()` - Summarize and ingest old exchanges
- `_retrieve_relevant_context()` - Vector search for relevant context
- `_generate_conversation_summary()` - Generate LLM summary (cached, with smart regeneration)
- `_should_regenerate_summary()` - Determine if summary needs regeneration
- `_clean_response()` - Clean LLM response formatting
- `end_session()` - Clear session and vector store

### Performance Optimizations in Chat Service
- **Cached Summary**: Store generated summary in session object
- **Smart Regeneration**: Only regenerate summary every 3-5 exchanges
- **Exchange Counter**: Track `last_summary_exchange_count` to determine when to regenerate
- **Fallback**: Use cached summary if regeneration fails

### Session Manager (Simplified Initial Implementation)
- `get_global_session()` - Get or create single global session
- `clear_global_session()` - Clear the global session
- `chat(message)` - Process chat message using global session

### Future Session Manager (For Multi-Session Expansion)
- `create_session(session_id)` - Create new session
- `get_session(session_id)` - Retrieve existing session (defaults to global if not provided)
- `clear_session(session_id)` - Clear specific session
- `clear_all_sessions()` - Clear all (shutdown)
- `get_active_sessions()` - Get count/list of active sessions

## CLI Integration

### New Command: `run start`
- Start FastAPI server using uvicorn
- Port: 8000
- Host: 127.0.0.1 (local only)
- Reload: Enabled for development
- Command: `python -m uvicorn src.api.main:app --host 127.0.0.1 --port 8000 --reload`

### Implementation in `run` file
- Add `run_start()` function
- Check virtual environment
- Execute uvicorn command
- Handle errors gracefully

## Configuration

### System Prompt
- Path: `src/demos/system_prompt.md`
- Loaded at startup
- Fallback prompt if file not found

### Collection Names
- Artifact RAG: `"persistant_docs"` (existing)
- Chat Context RAG: `"context_docs"` (from demo)

### Port
- Default: 8000
- Configurable via environment variable (optional)

## Testing Considerations

### Future Test Structure
- `src/tests/tests_api/` - API endpoint tests
- Mock RAG system and generators
- Test session management (single global session)
- Test error handling
- Test health check (lightweight)
- Test session clear endpoint

## Migration Notes

### From Demos to Production
- Extract `ContextChatSession` class â†’ `ChatService` with single global session
- Simplify session management (no session_id lookup initially)
- Remove demo-specific code (interactive mode, automated mode)
- Keep core logic (context management, RAG integration)
- Move system prompt loading to startup
- Add proper error handling and logging
- Add performance optimizations (cached summary)

### Backward Compatibility
- Existing generators unchanged (used as-is)
- Existing RAG system unchanged (used as-is)
- Demo files remain for reference

## Security Considerations

### Current (Local Only)
- No authentication/authorization
- No CORS (local only)
- No rate limiting

### Future Enhancements
- Authentication middleware
- CORS configuration
- Rate limiting
- Input sanitization

## Performance Considerations

### Initialization
- Heavy components (RAG, generators) initialized at startup
- Session creation lightweight (just dict entry)
- Vector store collections created on-demand per session

### Request Handling
- Async endpoints for concurrency
- Sync LLM calls wrapped in thread pool
- Session access: Direct reference to global session (no lookup needed initially)
- Future: Session lookup will be O(1) dictionary access when expanded

### Chat Performance Optimizations

#### Critical Performance Bottleneck
The `_generate_conversation_summary()` method in the original demo calls the LLM on every request, adding 1-2 seconds of latency and making real-time chat feel slow.

#### Current Performance
- Target: <5.0 seconds at 95th percentile (specification)
- Demo actual: 1.2-2.5 seconds p95 latency
- Bottleneck: LLM summary generation called on every request

#### Latency Breakdown (per request)
1. Context building: <10ms (memory access)
2. Vector search: <100ms (embedding + similarity)
3. **LLM summary generation: 1-2 seconds** (problematic bottleneck)
4. Main LLM chat call: 1-2 seconds (expected)
5. Response cleaning: <10ms

Total: 2-4 seconds (with bottleneck), 1-2 seconds (without it)

#### Optimization Strategy: Cached Summary with Smart Regeneration (Primary)
- Cache `conversation_summary` in the session
- Track when history changes using a hash or counter
- Regenerate summary only when history has significantly changed
- Use cached summary for subsequent requests
- **Expected latency: 1-2 seconds per request** (50% reduction)

#### Implementation Details
```
class ChatSession:
    - conversation_summary_cached: str
    - last_summary_exchange_count: int
    - should_regenerate_summary(): bool
        - Regenerate every 3-5 exchanges
        - Compare current exchange count to last_summary_exchange_count
        - Regenerate if difference > threshold
    - get_conversation_context():
        - Check if regeneration needed
        - If yes, regenerate and cache
        - Otherwise use cached summary
```

#### Additional Optimization Options (Future)

**Option 2: Asynchronous Summary Generation**
- Return chat response immediately
- Generate/update summary in background after response sent
- Use previous summary for next request
- Requires background task management
- Expected perceived latency: ~1 second (user doesn't wait)

**Option 3: Simple Heuristic (No LLM)**
- Extract key facts from history using simple parsing
- String concatenation for recent topics
- Only call LLM for summarization periodically (every N exchanges)
- Fastest option but lower quality summaries

**Option 4: Adaptive Frequency**
- Generate summary only when conversation shifts topics
- Use simple keyword tracking to detect topic changes
- Reduce LLM calls while maintaining quality
- Balance between speed and context quality

#### Performance Target
- Chat endpoint: 1-2 seconds per request (with caching)
- Meets specification: <5.0 seconds at 95th percentile
- Suitable for real-time chat experience

## Important Decisions

1. **Session Persistence**: Sessions kept until explicit clear or shutdown (no auto-expiration)
2. **System Prompt**: Same file as demo (`src/demos/system_prompt.md`)
3. **Collection Name**: `"context_docs"` for chat context (matches demo)
4. **No Request Validation**: Only Pydantic type validation, no custom constraints
5. **Full Artifact Response**: Frontend needs complete JSON structure
6. **Error Pass-through**: Generator errors sent directly to API response
7. **Health Check**: Lightweight checks only (cached initialization status, no runtime queries)
8. **Async Implementation**: Use async endpoints with thread pool for sync calls
9. **Startup Initialization**: Heavy components initialized once at startup
10. **Session Management**: Single global session initially (one user, one session at a time), expandable to multi-session later
11. **Chat Performance**: Implement cached summary with smart regeneration (regenerate every 3-5 exchanges) to achieve 1-2 second response times
12. **Session ID**: Optional parameter (defaults to global session), no client-side generation required initially

## Implementation Order

1. Update `requirements.txt` with FastAPI dependencies
2. Create `src/api/` directory structure
3. Create `src/llm_chat/` directory structure
4. Extract chat logic from demo into `llm_chat/` module with performance optimizations:
   - Add cached summary fields to session
   - Implement `_should_regenerate_summary()` method
   - Modify `_generate_conversation_summary()` to use cache
   - Update `get_conversation_context()` to use cached summary
5. Create Pydantic models
6. Implement dependency injection
7. Implement health endpoint
8. Implement artifact endpoints
9. Implement chat endpoint
10. Add `run start` command
11. Test all endpoints (including chat performance)
12. Update documentation

